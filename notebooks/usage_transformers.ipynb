{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "usage_transformers",
      "provenance": [],
      "authorship_tag": "ABX9TyNUEBS/kEZC5ELGFWYzup1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyoungkim1/ReadyToUseAI/blob/main/notebooks/usage_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxiXhiRd0Vb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56348fa0-cfb6-4ec1-ffce-0079a9fbd37b"
      },
      "source": [
        "!pip3 install -q transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Cgw5EsWK3QIm",
        "outputId": "e875579a-0c9b-4929-bc17-be09f2414c32"
      },
      "source": [
        "from transformers import BertTokenizerFast, TFGPT2LMHeadModel, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('kykim/gpt3-kor-small_based_on_gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('kykim/gpt3-kor-small_based_on_gpt2', pad_token_id=0)\n",
        "\n",
        "text = '인생이'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "input_ids = input_ids[:, 1:]  # remove cls token\n",
        "print(input_ids)\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(outputs)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[16082,  8018,     3]])\n",
            "tensor([[16082,  8018,     3, 14000, 14883, 13973, 14883, 14101,  2016,     3]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'인생이 너무 행복하고 행복하다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "phYuYh6o55wq",
        "outputId": "3b4da62d-8972-4b4c-d7a2-61c9cd214677"
      },
      "source": [
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "text = \"\"\"summarize: The t5 library serves primarily as code for reproducing the experiments in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In the paper, we demonstrate how to achieve state-of-the-art results on multiple NLP tasks using a text-to-text transformer pre-trained on a large text corpus.\n",
        "\n",
        "The bulk of the code in this repository is used for loading, preprocessing, mixing, and evaluating datasets. It also provides a way to fine-tune the pre-trained models released alongside the publication.\n",
        "\n",
        "The t5 library can be used for future model development by providing useful modules for training and fine-tuning (potentially huge) models on mixtures of text-to-text tasks.\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the t5 library is used for reproducing the experiments in Exploring the Limits of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBYgxzEP6IIC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}